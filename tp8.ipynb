{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "278381c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entreprises avec le plus de news :\n",
      "Tesla: 90 news\n",
      "Shell: 83 news\n",
      "Amazon: 52 news\n",
      "Apple: 49 news\n",
      "NVIDIA: 47 news\n",
      "Meta: 28 news\n",
      "Microsoft: 25 news\n",
      "Visa: 21 news\n",
      "Goldman_Sachs: 17 news\n",
      "Intel: 16 news\n",
      "Samsung: 15 news\n",
      "SAP: 15 news\n",
      "Oracle: 14 news\n",
      "BYD: 13 news\n",
      "Alphabet: 10 news\n",
      "SoftBank: 8 news\n",
      "TotalEnergies: 8 news\n",
      "Cisco: 7 news\n",
      "Toyota: 7 news\n",
      "Alibaba: 6 news\n",
      "AMD: 6 news\n",
      "Hyundai: 5 news\n",
      "IBM: 5 news\n",
      "Siemens: 5 news\n",
      "ASML: 4 news\n",
      "Adobe: 3 news\n",
      "Nintendo: 3 news\n",
      "Sony: 3 news\n",
      "ExxonMobil: 2 news\n",
      "Johnson_&_Johnson: 2 news\n",
      "Reliance_Industries: 2 news\n",
      "Tencent: 2 news\n",
      "Baidu: 1 news\n",
      "Pfizer: 1 news\n",
      "Qualcomm: 1 news\n",
      "Tata_Consultancy_Services: 1 news\n",
      "ICBC: 0 news\n",
      "JP_Morgan: 0 news\n",
      "Louis_Vuitton_(LVMH): 0 news\n",
      "[2025-05-17 12:52:50] Analyse pour Tesla\n",
      "[2025-05-17 12:52:50] Téléchargement des prix pour Tesla\n",
      "[2025-05-17 12:52:50] Chargement des news depuis JSONS\\Tesla_news.json\n",
      "[2025-05-17 12:52:50] Chargement du modèle depuis ./ProsusAI_finetuned\n",
      "[2025-05-17 12:53:46] Chargement du modèle depuis ./finbert_finetuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliot\\AppData\\Local\\Temp\\ipykernel_10472\\1982536510.py:153: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-17 12:54:40] Analyse pour Shell\n",
      "[2025-05-17 12:54:40] Téléchargement des prix pour Shell\n",
      "[2025-05-17 12:54:40] Chargement des news depuis JSONS\\Shell_news.json\n",
      "[2025-05-17 12:54:40] Chargement du modèle depuis ./ProsusAI_finetuned\n",
      "[2025-05-17 12:55:38] Chargement du modèle depuis ./finbert_finetuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliot\\AppData\\Local\\Temp\\ipykernel_10472\\1982536510.py:153: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta, time\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from collections import defaultdict\n",
    "from matplotlib.lines import Line2D\n",
    "import glob\n",
    "from bisect import bisect_left\n",
    "\n",
    "companies = {\n",
    "    \"Apple\": \"AAPL\", \"Microsoft\": \"MSFT\", \"Amazon\": \"AMZN\", \"Alphabet\": \"GOOGL\", \"Meta\": \"META\",\n",
    "    \"Tesla\": \"TSLA\", \"NVIDIA\": \"NVDA\", \"Samsung\": \"005930.KS\", \"Tencent\": \"TCEHY\", \"Alibaba\": \"BABA\",\n",
    "    \"IBM\": \"IBM\", \"Intel\": \"INTC\", \"Oracle\": \"ORCL\", \"Sony\": \"SONY\", \"Adobe\": \"ADBE\",\n",
    "    \"Netflix\": \"NFLX\", \"AMD\": \"AMD\", \"Qualcomm\": \"QCOM\", \"Cisco\": \"CSCO\", \"JP Morgan\": \"JPM\",\n",
    "    \"Goldman Sachs\": \"GS\", \"Visa\": \"V\", \"Johnson & Johnson\": \"JNJ\", \"Pfizer\": \"PFE\",\n",
    "    \"ExxonMobil\": \"XOM\", \"ASML\": \"ASML.AS\", \"SAP\": \"SAP.DE\", \"Siemens\": \"SIE.DE\",\n",
    "    \"Louis Vuitton (LVMH)\": \"MC.PA\", \"TotalEnergies\": \"TTE.PA\", \"Shell\": \"SHEL.L\",\n",
    "    \"Baidu\": \"BIDU\", \"JD.com\": \"JD\", \"BYD\": \"BYDDY\", \"ICBC\": \"1398.HK\", \"Toyota\": \"TM\",\n",
    "    \"SoftBank\": \"9984.T\", \"Nintendo\": \"NTDOY\", \"Hyundai\": \"HYMTF\", \"Reliance Industries\": \"RELIANCE.NS\",\n",
    "    \"Tata Consultancy Services\": \"TCS.NS\"\n",
    "}\n",
    "\n",
    "def log(message):\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n",
    "def convert_utc_to_ny(timestamp_str):\n",
    "    utc_dt = datetime.fromisoformat(timestamp_str.replace(\"Z\", \"+00:00\"))\n",
    "    ny_tz = pytz.timezone(\"America/New_York\")\n",
    "    ny_dt = utc_dt.astimezone(ny_tz)\n",
    "    return ny_dt.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "def get_texts_timestamps(news_data):\n",
    "    texts = []\n",
    "    timestamps = []\n",
    "    for day_articles in news_data.values():\n",
    "        for article in day_articles:\n",
    "            ts = convert_utc_to_ny(article['publishedAt'])\n",
    "            text = clean_text(article.get(\"title\", \"\") + \" \" + article.get(\"description\", \"\"))\n",
    "            texts.append(text)\n",
    "            timestamps.append(ts)\n",
    "    return texts, timestamps\n",
    "\n",
    "def get_sentiments(model_path, texts):\n",
    "    log(f\"Chargement du modèle depuis {model_path}\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    sentiments = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        sentiments.append(pred)\n",
    "    return sentiments\n",
    "\n",
    "def align_timestamps(timestamps):\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "    aligned = []\n",
    "    ny_tz = pytz.timezone(\"America/New_York\")\n",
    "    calendar = USFederalHolidayCalendar()\n",
    "    holidays = set(h.date() for h in calendar.holidays(start=\"2025-01-01\", end=\"2025-12-31\"))\n",
    "\n",
    "    for ts in timestamps:\n",
    "        ts = ts.astimezone(ny_tz)\n",
    "        local_date = ts.date()\n",
    "        local_time = ts.time()\n",
    "        weekday = ts.weekday()  # 0=lundi, 6=dimanche\n",
    "\n",
    "        # Cas 1 : weekend → reculer au vendredi précédent\n",
    "        if weekday == 5:  # samedi\n",
    "            target = ts - timedelta(days=1)\n",
    "        elif weekday == 6:  # dimanche\n",
    "            target = ts - timedelta(days=2)\n",
    "        # Cas 2 : jour férié\n",
    "        elif local_date in holidays:\n",
    "            target = ts - timedelta(days=1)\n",
    "            while target.date() in holidays or target.weekday() >= 5:\n",
    "                target -= timedelta(days=1)\n",
    "        # Cas 3 : jour de marché\n",
    "        else:\n",
    "            if datetime.strptime(\"09:30\", \"%H:%M\").time() <= local_time < datetime.strptime(\"15:00\", \"%H:%M\").time():\n",
    "                aligned.append(ts.replace(minute=0, second=0, microsecond=0))\n",
    "                continue\n",
    "            elif local_time >= datetime.strptime(\"15:00\", \"%H:%M\").time():\n",
    "                aligned.append(ts.replace(hour=15, minute=0, second=0, microsecond=0))\n",
    "                continue\n",
    "            else:\n",
    "                target = ts - timedelta(days=1)\n",
    "\n",
    "        # S'assurer que le timestamp aligné est localisé New York\n",
    "        aligned_dt = datetime.combine(target.date(), time(15, 0))\n",
    "        aligned.append(ny_tz.localize(aligned_dt))\n",
    "\n",
    "    return aligned\n",
    "\n",
    "\n",
    "def plot_comparison(df, sentiments_a, sentiments_b, timestamps, title_a, title_b):\n",
    "    aligned_ts = align_timestamps(timestamps)\n",
    "\n",
    "    def group_by_time(ts_list, sentiments_list):\n",
    "        grouped = defaultdict(list)\n",
    "        for t, s in zip(ts_list, sentiments_list):\n",
    "            grouped[t].append(s)\n",
    "        return grouped\n",
    "\n",
    "    grouped_a = group_by_time(aligned_ts, sentiments_a)\n",
    "    grouped_b = group_by_time(aligned_ts, sentiments_b)\n",
    "\n",
    "    def plot_sub(df, ax, grouped, title):\n",
    "        df = df.set_index(\"Datetime\" if \"Datetime\" in df.columns else df.columns[0])\n",
    "        index_list = df.index.to_list()\n",
    "        ax.plot(df.index, df[\"Close\"], label=\"Price\", color=\"black\")\n",
    "        colors = {0: \"red\", 1: \"orange\", 2: \"green\"}\n",
    "        offset = 0.5\n",
    "\n",
    "        for t, s_list in grouped.items():\n",
    "            pos = bisect_left(index_list, t)\n",
    "            if pos == len(index_list):\n",
    "                continue\n",
    "            nearest = index_list[pos] if abs(index_list[pos] - t) <= timedelta(minutes=90) else None\n",
    "            if nearest:\n",
    "                price = df.loc[nearest][\"Close\"]\n",
    "                for i, s in enumerate(s_list):\n",
    "                    ax.scatter(nearest, price + i * offset, color=colors[s], s=60)\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_ylabel(\"Price\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "    plot_sub(df, ax1, grouped_a, title_a)\n",
    "    plot_sub(df, ax2, grouped_b, title_b)\n",
    "\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Positive', markerfacecolor='green', markersize=10),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Neutral', markerfacecolor='orange', markersize=10),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Negative', markerfacecolor='red', markersize=10),\n",
    "        Line2D([0], [0], color='black', lw=2, label='Price')\n",
    "    ]\n",
    "    ax2.legend(handles=legend_elements)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def run_analysis(company, json_path, model_path_a, model_path_b):\n",
    "    log(f\"Téléchargement des prix pour {company}\")\n",
    "    ticker_symbol = companies.get(company, company)\n",
    "    ticker = yf.Ticker(ticker_symbol)\n",
    "    df = ticker.history(start=\"2025-01-01\", end=\"2025-04-15\", interval=\"60m\")\n",
    "    df = df.reset_index() if 'Datetime' not in df.columns else df\n",
    "\n",
    "    log(f\"Chargement des news depuis {json_path}\")\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        news_data = json.load(f)\n",
    "\n",
    "    texts, timestamps = get_texts_timestamps(news_data)\n",
    "    sentiments_a = get_sentiments(model_path_a, texts)\n",
    "    sentiments_b = get_sentiments(model_path_b, texts)\n",
    "    plot_comparison(df, sentiments_a, sentiments_b, timestamps, \"Model A\", \"Model B\")\n",
    "\n",
    "def count_news_per_company(json_dir):\n",
    "    summary = {}\n",
    "    for path in glob.glob(os.path.join(json_dir, \"*_news.json\")):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        total = sum(len(v) for v in data.values())\n",
    "        company = os.path.basename(path).replace(\"_news.json\", \"\")\n",
    "        summary[company] = (total, path)\n",
    "    return {k: v for k, v in sorted(summary.items(), key=lambda x: x[1][0], reverse=True)}\n",
    "def test_run_analysis():\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Pour exécuter sans afficher\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "\n",
    "    news_counts = count_news_per_company(\"JSONS\")\n",
    "    if not news_counts:\n",
    "        raise RuntimeError(\"Aucun fichier JSON valide trouvé dans le dossier 'JSONS'.\")\n",
    "\n",
    "    company, (_, json_path) = next(iter(news_counts.items()))\n",
    "    ticker_symbol = companies.get(company, company)\n",
    "    log(f\"⚙️ Test sur {company} ({ticker_symbol})\")\n",
    "\n",
    "    # Charger données\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        news_data = json.load(f)\n",
    "    texts, timestamps = get_texts_timestamps(news_data)\n",
    "\n",
    "    sentiments_a = get_sentiments(\"./ProsusAI_finetuned\", texts)\n",
    "    sentiments_b = get_sentiments(\"./finbert_finetuned\", texts)\n",
    "\n",
    "    assert len(texts) == len(timestamps) == len(sentiments_a) == len(sentiments_b), \\\n",
    "        \"❌ Mismatch entre les longueurs des listes.\"\n",
    "\n",
    "    for s in sentiments_a + sentiments_b:\n",
    "        assert s in {0, 1, 2}, f\"❌ Label de sentiment invalide : {s}\"\n",
    "\n",
    "    aligned_ts = align_timestamps(timestamps)\n",
    "    grouped = defaultdict(list)\n",
    "    for t, s in zip(aligned_ts, sentiments_a):  # on teste pour le modèle A\n",
    "        grouped[t].append(s)\n",
    "\n",
    "    # Charger prix\n",
    "    df = yf.Ticker(ticker_symbol).history(start=\"2025-01-01\", interval=\"60m\")\n",
    "    df = df.reset_index() if 'Datetime' not in df.columns else df\n",
    "    df = df.set_index(\"Datetime\" if \"Datetime\" in df.columns else df.columns[0])\n",
    "    index_list = df.index.to_list()\n",
    "\n",
    "    # Tracer sur figure cachée\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(df.index, df[\"Close\"], color=\"black\")\n",
    "\n",
    "    from bisect import bisect_left\n",
    "    colors = {0: \"red\", 1: \"orange\", 2: \"green\"}\n",
    "    total_points = 0\n",
    "\n",
    "    for t, s_list in grouped.items():\n",
    "        pos = bisect_left(index_list, t)\n",
    "        if pos == len(index_list):\n",
    "            continue\n",
    "        nearest = index_list[pos] if abs(index_list[pos] - t) <= timedelta(minutes=60) else None\n",
    "        if nearest:\n",
    "            price = df.loc[nearest][\"Close\"]\n",
    "            for i, s in enumerate(s_list):\n",
    "                ax.scatter(nearest, price + i * 0.5, color=colors[s], s=60)\n",
    "                total_points += 1\n",
    "\n",
    "    expected = len(sentiments_a)\n",
    "    missing_points = []\n",
    "\n",
    "    for t, s_list in grouped.items():\n",
    "        pos = bisect_left(index_list, t)\n",
    "        nearest = None\n",
    "        if pos < len(index_list) and abs(index_list[pos] - t) <= timedelta(minutes=60):\n",
    "            nearest = index_list[pos]\n",
    "        elif pos > 0 and abs(index_list[pos - 1] - t) <= timedelta(minutes=60):\n",
    "            nearest = index_list[pos - 1]\n",
    "\n",
    "        if nearest:\n",
    "            price = df.loc[nearest][\"Close\"]\n",
    "            for i, s in enumerate(s_list):\n",
    "                ax.scatter(nearest, price + i * 0.5, color=colors[s], s=60)\n",
    "                total_points += 1\n",
    "        else:\n",
    "            # Ajouter les timestamps manquants\n",
    "            for s in s_list:\n",
    "                missing_points.append((t, s))\n",
    "\n",
    "    # Affichage d'erreur s'il manque des points\n",
    "    if len(missing_points) > 0:\n",
    "        print(f\"❌ Seulement {total_points}/{expected} points ont été affichés.\")\n",
    "        print(\"❌ Timestamps sans correspondance dans les données de prix (±60 minutes) :\")\n",
    "        for t, s in missing_points:\n",
    "            print(f\"   - {t} (sentiment: {s})\")\n",
    "    else:\n",
    "        log(f\"✅ Tous les {expected} points ont bien été affichés.\")\n",
    "\n",
    "        log(\"✔ Résumé des prédictions :\")\n",
    "        print(\"  ProsusAI:\", Counter(sentiments_a))\n",
    "        print(\"  Fine-tuné:\", Counter(sentiments_b))\n",
    "        log(f\"✅ Tous les tests sont passés et {total_points}/{expected} points ont bien été affichés.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #test_run_analysis()\n",
    "\n",
    "    news_counts = count_news_per_company(\"JSONS\")\n",
    "    print(\"Entreprises avec le plus de news :\")\n",
    "    for company, (count, _) in news_counts.items():\n",
    "        print(f\"{company}: {count} news\")\n",
    "\n",
    "    top_2_companies = list(news_counts.items())[:2]\n",
    "    for company, (_, path) in top_2_companies:\n",
    "        log(f\"Analyse pour {company}\")\n",
    "        run_analysis(\n",
    "            company=company,\n",
    "            json_path=path,\n",
    "            model_path_a=\"./ProsusAI_finetuned\",\n",
    "            model_path_b=\"./finbert_finetuned\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
