# -*- coding: utf-8 -*-
"""Python TP2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KhHu__OIYuJ8K8SwjJtYc478Ax32P3Cc

# 1. Financial profiles clustering
"""

import yfinance as yf
import pandas as pd
from datetime import datetime
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.manifold import TSNE
from scipy.spatial.distance import cdist,squareform, pdist
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import glob

def preprocess_financial_clustering(file_path):
    df = pd.read_csv(file_path)

    # Sélection des colonnes pertinentes
    financial_features = ['forwardPE', 'beta', 'priceToBook', 'returnOnEquity']
    df_filtered = df[financial_features].dropna()

    # Sauvegarde des noms pour plus tard
    names = df_filtered.index
    df_features = df_filtered[financial_features]

    # Standardisation
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(df_features)

    return pd.DataFrame(data_scaled, columns=financial_features), names

def elbow_method(data, max_k=10):
    inertias = []
    for k in range(1, max_k+1):
        kmeans = KMeans(n_clusters=k, random_state=0)
        kmeans.fit(data)
        inertias.append(kmeans.inertia_)

    plt.figure(figsize=(8,5))
    plt.plot(range(1, max_k+1), inertias, marker='o')
    plt.title('Méthode du coude - Inertie en fonction de K')
    plt.xlabel('Nombre de clusters (K)')
    plt.ylabel('Inertie')
    plt.grid(True)
    plt.show()

def do_kmeans_clustering(data, names, n_clusters=3):
    kmeans = KMeans(n_clusters=n_clusters, random_state=0)
    clusters = kmeans.fit_predict(data)

    # Ajout de la colonne clusters
    df_clustered = pd.DataFrame(data, columns=data.columns)
    df_clustered['Cluster'] = clusters
    df_clustered['Name'] = names.values

    # Visualisation t-SNE
    tsne = TSNE(n_components=2, random_state=42)
    tsne_result = tsne.fit_transform(data)

    plt.figure(figsize=(8, 6))
    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=clusters, cmap='tab10', s=50)
    plt.title("t-SNE des clusters financiers")
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    plt.grid(True)
    plt.show()

    return df_clustered

# Charger et prétraiter
file_path = 'ratios_financiers.csv'
data_scaled, names = preprocess_financial_clustering(file_path)

# Choisir le bon K avec la méthode du coude
elbow_method(data_scaled)

# Appliquer KMeans (ex: k = 3)
result_df = do_kmeans_clustering(data_scaled, names, n_clusters=3)

"""# 2. Risk profiles clustering"""

def preprocess_risk_clustering(file_path):
    df = pd.read_csv(file_path)

    risk_features = ['debtToEquity', 'currentRatio', 'quickRatio', 'returnOnAssets']
    df_filtered = df[risk_features + ['Unnamed: 0']].dropna()

    names = df_filtered['Unnamed: 0']
    df_features = df_filtered[risk_features]

    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(df_features)

    return pd.DataFrame(data_scaled, columns=risk_features), names

def do_hierarchical_clustering(data, names, n_clusters=3):
    model = AgglomerativeClustering(n_clusters=n_clusters)
    clusters = model.fit_predict(data)

    df_clustered = pd.DataFrame(data, columns=data.columns)
    df_clustered['Cluster'] = clusters
    df_clustered['Name'] = names.values

    return df_clustered

def plot_dendrogram(data, method='ward'):
    linked = linkage(data, method=method)

    plt.figure(figsize=(12, 6))
    dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=False)
    plt.title("Dendrogramme des profils de risque")
    plt.xlabel("Entreprises")
    plt.ylabel("Distance")
    plt.grid(True)
    plt.show()

# Prétraitement
data_risk_scaled, names_risk = preprocess_risk_clustering("ratios_financiers.csv")

# Affichage du dendrogramme pour estimer le bon nombre de clusters
plot_dendrogram(data_risk_scaled)

# Clustering hiérarchique avec le nombre choisi
result_risk_df = do_hierarchical_clustering(data_risk_scaled, names_risk, n_clusters=3)

"""# 3. Daily returns correlations clustering"""

def prepare_returns_data(folder_path):
    filepaths = glob.glob(f"{folder_path}/*.csv")
    returns_dict = {}

    for path in filepaths:
        df = pd.read_csv(path)
        company_name = path.split("/")[-1].replace(".csv", "")

        if "Rendement" in df.columns:
            returns_dict[company_name] = df["Rendement"]

    returns_df = pd.DataFrame(returns_dict)

    # Remplissage des valeurs manquantes avec la moyenne par entreprise
    returns_df = returns_df.fillna(returns_df.mean())

    return returns_df

def plot_correlation_dendrogram(returns_df):
    # Corrélation entre entreprises
    corr_matrix = returns_df.corr()

    # Distance = 1 - corrélation
    distance_matrix = 1 - corr_matrix

    linked = linkage(distance_matrix, method='ward')

    plt.figure(figsize=(12, 6))
    dendrogram(linked, labels=returns_df.columns, orientation='top', distance_sort='descending')
    plt.title("Dendrogramme des corrélations de rendements journaliers")
    plt.xlabel("Entreprises")
    plt.ylabel("Distance (1 - Corrélation)")
    plt.grid(True)
    plt.show()

# Charger les rendements journaliers
returns_df = prepare_returns_data("Companies_Historical_Data/")

# Tracer le dendrogramme de corrélation
plot_correlation_dendrogram(returns_df)

"""# 4. Evaluation et comparaison des algorithmes"""

def do_dbscan_clustering(data, eps=0.5, min_samples=5):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    clusters = dbscan.fit_predict(data)

    df_clustered = pd.DataFrame(data, columns=data.columns)
    df_clustered['Cluster'] = clusters

    return df_clustered

from sklearn.metrics import silhouette_score

def evaluate_clustering(data, clustering_labels):
    if len(set(clustering_labels)) > 1:  # Silhouette Score nécessite au moins 2 clusters
        score = silhouette_score(data, clustering_labels)
    else:
        score = -1  # Mauvais clustering si un seul cluster détecté

    return score

def compare_algorithms(data_financial, data_risk, data_returns):
    results = []

    # K-Means
    kmeans_fin = KMeans(n_clusters=3, random_state=0).fit_predict(data_financial)
    kmeans_risk = KMeans(n_clusters=3, random_state=0).fit_predict(data_risk)
    kmeans_returns = KMeans(n_clusters=3, random_state=0).fit_predict(data_returns)

    results.append(["K-Means", "Finance", evaluate_clustering(data_financial, kmeans_fin)])
    results.append(["K-Means", "Risk", evaluate_clustering(data_risk, kmeans_risk)])
    results.append(["K-Means", "Returns", evaluate_clustering(data_returns, kmeans_returns)])

    # Clustering Hiérarchique
    hier_fin = AgglomerativeClustering(n_clusters=3).fit_predict(data_financial)
    hier_risk = AgglomerativeClustering(n_clusters=3).fit_predict(data_risk)
    hier_returns = AgglomerativeClustering(n_clusters=3).fit_predict(data_returns)

    results.append(["Hierarchical", "Finance", evaluate_clustering(data_financial, hier_fin)])
    results.append(["Hierarchical", "Risk", evaluate_clustering(data_risk, hier_risk)])
    results.append(["Hierarchical", "Returns", evaluate_clustering(data_returns, hier_returns)])

    # DBSCAN
    dbscan_fin = do_dbscan_clustering(data_financial)['Cluster']
    dbscan_risk = do_dbscan_clustering(data_risk)['Cluster']
    dbscan_returns = do_dbscan_clustering(data_returns)['Cluster']

    results.append(["DBSCAN", "Finance", evaluate_clustering(data_financial, dbscan_fin)])
    results.append(["DBSCAN", "Risk", evaluate_clustering(data_risk, dbscan_risk)])
    results.append(["DBSCAN", "Returns", evaluate_clustering(data_returns, dbscan_returns)])

    return pd.DataFrame(results, columns=["Algorithm", "Dataset", "Silhouette Score"])

# Charger les données des 3 types
data_financial, _ = preprocess_financial_clustering(file_path)
data_risk, _ = preprocess_risk_clustering(file_path)
data_returns = prepare_returns_data("Companies_Historical_Data/")

# Comparer les algorithmes
results_df = compare_algorithms(data_financial, data_risk, data_returns)

print(results_df)

"""DBSCAN a produit les meilleurs résultats pour le clustering des rendements journaliers avec un Silhouette Score élevé de **0.7446**. Cela suggère que DBSCAN a trouvé des groupes significatifs dans les données de rendement, malgré la faible performance pour les autres ensembles de données.

Le score de K-Means sur le risque (**0.5573**) et sur la finance (**0.2999**) est raisonnable, mais il ne parvient pas à bien capturer les clusters pour les rendements. Il pourrait être utile de réajuster les paramètres de K-Means ou de tester d’autres techniques pour améliorer les résultats sur les rendements.

Clustering Hiérarchique a aussi montré des résultats intéressants, particulièrement pour les données financières (**0.5653**). Il semble que cet algorithme fonctionne mieux avec des ensembles de données plus structurés.

**Conclusion** :\
DBSCAN est le meilleur choix pour les rendements, mais pour les données financières et de risque, K-Means ou Clustering Hiérarchique semblent plus adaptés.
"""