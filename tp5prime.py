# -*- coding: utf-8 -*-
"""TP5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wUfHB_JN7d9IHBfKzywKe5nPier2ahNH
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import seaborn as sns

"""# 1. Chargement des datasets (par entreprise)"""

# Chargement des prix close
def load_close_prices(file_path):
    df = pd.read_csv(file_path)
    return df[['Close']]

# Standardisation + split
def scale_and_split(data, split_ratio=0.8):
    scaler = MinMaxScaler()
    train_size = int(len(data) * split_ratio)
    train_data = data[:train_size]
    test_data = data[train_size:]
    train_scaled = scaler.fit_transform(train_data)
    test_scaled = scaler.transform(test_data)
    return train_scaled, test_scaled, scaler

# Création X et Y
def create_target_features(df_scaled, n_days=30):
    x = []
    y = []
    for i in range(n_days, len(df_scaled)):
        x.append(df_scaled[i - n_days:i, 0])
        y.append(df_scaled[i, 0])
    return np.array(x), np.array(y)

# Pipeline complet pour un fichier
def prepare_dataset(file_path, n_days=30, split_ratio=0.8):
    df = load_close_prices(file_path)
    train_scaled, test_scaled, scaler = scale_and_split(df, split_ratio)
    x_train, y_train = create_target_features(train_scaled, n_days)
    x_test, y_test = create_target_features(test_scaled, n_days)
    return x_train, y_train, x_test, y_test, scaler

# Paramètres
data_folder = "Companies_Historical_Data"  # Dossier contenant les CSV
output_folder = "datasets"  # Dossier où on sauvegarde les fichiers
os.makedirs(output_folder, exist_ok=True)

n_days = 30
split_ratio = 0.8

# Lister tous les fichiers CSV
file_list = [f for f in os.listdir(data_folder) if f.endswith(".csv")]

# Pour chaque fichier, on prépare et sauvegarde les données
for filename in file_list:
    name = os.path.splitext(filename)[0]  # "apple.csv" -> "apple"
    file_path = os.path.join(data_folder, filename)

    # Préparation des données (fonction que tu as déjà)
    x_train, y_train, x_test, y_test, scaler = prepare_dataset(file_path, n_days, split_ratio)

    # Sauvegarde sous forme de fichiers .npy et .pkl
    np.save(os.path.join(output_folder, f"{name}_x_train.npy"), x_train)
    np.save(os.path.join(output_folder, f"{name}_y_train.npy"), y_train)
    np.save(os.path.join(output_folder, f"{name}_x_test.npy"), x_test)
    np.save(os.path.join(output_folder, f"{name}_y_test.npy"), y_test)
    joblib.dump(scaler, os.path.join(output_folder, f"{name}_scaler.pkl"))

    print(f"{name} : fichiers sauvegardés.")

def load_data(filepath, target_col, exog_cols=None):
    df = pd.read_csv(filepath, parse_dates=True, index_col=0)
    y = df[[target_col]].dropna()
    scaler_y = MinMaxScaler()
    y_scaled = scaler_y.fit_transform(y)

    if exog_cols:
        exog = df[exog_cols].fillna(method='ffill')
        scaler_exog = MinMaxScaler()
        exog_scaled = scaler_exog.fit_transform(exog)
        full_data = np.concatenate([y_scaled, exog_scaled], axis=1)
    else:
        full_data = y_scaled
        scaler_exog = None

    return df, full_data, scaler_y, scaler_exog

def create_sequences(data, window):
    X, y = [], []
    for i in range(len(data) - window):
        X.append(data[i:i + window])
        y.append(data[i + window, 0])
    return np.array(X), np.array(y)

def build_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(64, input_shape=input_shape))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

def plot_predictions(df, y_true, y_pred, scaler_y, window, title="Prévision", save_path=None):
    y_true_inv = scaler_y.inverse_transform(y_true.reshape(-1, 1)).ravel()
    y_pred_inv = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).ravel()
    idx = df.index[-len(y_true):]
    
    plt.figure(figsize=(12, 6))
    plt.plot(idx, y_true_inv, label="Réel")
    plt.plot(idx, y_pred_inv, label="Prévu")
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""# 3. entraînement et comparaison sur toutes les entreprises

representation
"""

# Modèles à tester
model_types = ["MLP", "RNN", "LSTM"]

# Fichiers dans le dossier datasets
file_list = sorted([f for f in os.listdir("datasets") if f.endswith("_x_train.npy")])
entreprises = [f.split("_")[0] for f in file_list]

for entreprise in entreprises:
    print(f"Traitement de l’entreprise : {entreprise.upper()}")

    # Charger les données
    x_train, y_train, x_test, y_test, scaler = load_dataset(entreprise)

    for model_type in model_types:
        print(f"\n--- Modèle : {model_type} ---")

        # Entraînement
        model = train_model(
            model_type=model_type,
            X_train=x_train,
            y_train=y_train,
            input_shape=(x_train.shape[1], 1),
            hidden_dims=[50],
            activation="tanh" if model_type != "MLP" else "relu",
            dropout_rate=0.2,
            optimizer="Adam",
            learning_rate=0.001,
            epochs=20,
            batch_size=32
        )

        # Prédiction + Affichage
        predict(model, x_test, y_test, scaler, model_type=model_type, entreprise_name=entreprise)